{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlHL+hEfsu4wWTmoRAhXGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rida-manzoor/ML/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **ensemble methods** in machine learning combine the insights obtained from multiple learning models to facilitate accurate and improved decisions. These methods follow the same principle.\n",
        "\n",
        "In learning models, noise, variance, and bias are the major sources of error. The ensemble methods in machine learning help minimize these error-causing factors, thereby ensuring the accuracy and stability of machine learning (ML) algorithms."
      ],
      "metadata": {
        "id": "RH1tAfVLsE3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Techniques\n",
        "\n",
        "1. **Max Voting**\n",
        "\n",
        "  In Max Voting, multiple models make predictions, and the final prediction is determined by the majority vote.\n",
        "\n",
        "  **Example:** Suppose you have three models A, B, and C. If models A and B predict class 1, and model C predicts class 0, the Max Voting ensemble would choose class 1 as the final prediction.\n",
        "\n",
        "2. **Averaging**\n",
        "\n",
        "  Averaging involves aggregating predictions by taking the average of individual model predictions.\n",
        "\n",
        "  **Example:** If models A, B, and C predict values of 0.8, 0.7, and 0.9, respectively, the averaged prediction would be\n",
        "  \n",
        "  $\n",
        "  \\frac{0.8 + 0.7 + 0.9}{3}\n",
        "  = 0.8\n",
        "  $\n",
        "\n",
        "3. **Weighted Average**\n",
        "\n",
        "  Similar to averaging, but each model's prediction is weighted differently.\n",
        "\n",
        "  **Example:** If models A, B, and C have weights 0.3, 0.4, and 0.3, respectively, the weighted average would be\n",
        "\n",
        "  $\n",
        "  0.3 * pred_A + 0.4 * pred_B + 0.3 * pred_C\n",
        "  $\n",
        "\n",
        "4. **Rank Average**\n",
        "\n",
        "  In Rank Average, predictions are ranked, and the final prediction is determined by the average rank.\n",
        "  \n",
        "  **Example:** If models A, B, and C rank an item 2nd, 1st, and 3rd, respectively, the average rank would be\n",
        "  \n",
        "   $\n",
        "   \\frac{2 +1 +3}{3}\n",
        "  = 2\n",
        "  $"
      ],
      "metadata": {
        "id": "4qw2fIOrLXZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advance Techniques\n",
        "\n",
        "1. **Stacking**\n",
        "\n",
        "  Stacking is a widely used ensemble machine learning technique designed to enhance model performance by combining the predictions of multiple base models to create a new model. This approach involves training several models to address similar problems and leveraging their collective output to construct a more effective model.\n",
        "\n",
        "**Key Components of Stacking:**\n",
        "\n",
        "1. **Input and Output:**\n",
        "   - The algorithm takes the outputs of sub-models (base models) as input and endeavors to learn how to optimally combine these input predictions to generate an improved output prediction.\n",
        "\n",
        "2. **Model Composition:**\n",
        "   - Stacking, also known as stacked generalization, extends the Model Averaging Ensemble technique. It involves multiple sub-models, each contributing to the new model according to their performance weights. The new model is stacked on top of the others, which gives stacking its name.\n",
        "\n",
        "   ![Stacking Architecture](https://editor.analyticsvidhya.com/uploads/39725Stacking.png)\n",
        "\n",
        "**Architecture of a Stacking Model:**\n",
        "\n",
        "The architecture of a stacking model comprises:\n",
        "\n",
        "1. **Level-0 Models (Base-Models):**\n",
        "   - These are models fitted on the training data, and their predictions are compiled.\n",
        "\n",
        "2. **Level-1 Model (Meta-Model):**\n",
        "   - The meta-model learns how to optimally combine the predictions of the base models to produce an enhanced final prediction.\n",
        "\n",
        "**Training Process:**\n",
        "\n",
        "- The meta-model is trained on predictions made by the base models on out-of-sample data.\n",
        "- Out-of-sample data, not used to train the base models, is fed to the base models for prediction.\n",
        "- The resulting predictions, along with the expected outputs, form the input and output pairs for training the meta-model.\n",
        "\n",
        "**Preparing Training Dataset for Meta-Model:**\n",
        "\n",
        "- Commonly, k-fold cross-validation of the base models is employed to create the training dataset for the meta-model.\n",
        "- Out-of-fold predictions serve as the basis for the meta-model's training dataset.\n",
        "\n",
        "**Data Input to Meta-Model:**\n",
        "\n",
        "- The inputs to the base models (e.g., elements of the training data) may be included in the training data for the meta-model.\n",
        "- This provides additional context to the meta-model on how to effectively combine predictions from the base models.\n",
        "\n",
        "**Training Process:**\n",
        "\n",
        "- Once the training dataset is prepared, the meta-model is trained independently on this dataset.\n",
        "- The base models are trained on the entire original training dataset.\n",
        "\n",
        "**Note:** The output from base models used as input to the meta-model may vary, such as real values for regression or probability values/class labels for classification. This flexibility makes stacking suitable for various problem types.\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "3. **Bootstrap Sampling**\n",
        "\n",
        "4. **Bagging**\n",
        "\n",
        "5. **Boosting**"
      ],
      "metadata": {
        "id": "L5YtK60eLtTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a set of base models\n",
        "base_models = [('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "               ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
        "               ('svc', SVC(probability=True))]\n",
        "\n",
        "# Create a StackingClassifier with a meta-model (Logistic Regression)\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "\n",
        "# Fit the stacking model on the training data\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "# Evaluate the stacking model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4FH1xyqXTCc",
        "outputId": "1e6ad6eb-10a8-491d-af25-affe20d4d5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Blending**\n",
        "\n",
        "Blending is one of the many ensemble machine-learning techniques that make use of a machine-learning model to figure out how to blend predictions from several ensemble member models in the most effective way. It is similar to Stacking (another ensemble learning technique). Hence it might get interchanged in some illustrations like magazines and research papers.\n",
        "\n",
        "The architecture of a blending and most of the ensemble learning models consists of two or more base models, also known as level-0 models, and a meta-model, or level-1 model, which integrates the predictions of the base models. The meta-model (main model) is trained on the predictions made by the base models on out-of-sample data."
      ],
      "metadata": {
        "id": "_VJU2y_alTB2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkbJSM-Js5Rj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Bootstrap sampling**\n",
        "\n",
        "  is a resampling technique that involves generating multiple datasets by randomly sampling with replacement from the original dataset. The term \"bootstrap\" is derived from the phrase \"pulling oneself up by one's bootstraps,\" and in statistics, it refers to the idea of creating new samples from the observed data to estimate the properties of a population.\n",
        "\n",
        "Here are the key steps involved in bootstrap sampling:\n",
        "\n",
        "1. **Original Dataset:**\n",
        "   - Begin with a dataset containing \\(n\\) observations.\n",
        "\n",
        "2. **Random Sampling with Replacement:**\n",
        "   - Draw \\(n\\) samples from the dataset with replacement. This means that each observation in the original dataset has an equal chance of being selected in each draw, and it's possible for the same observation to be selected multiple times.\n",
        "\n",
        "3. **Create Bootstrap Sample:**\n",
        "   - The set of \\(n\\) observations obtained through random sampling with replacement constitutes a bootstrap sample. This sample is typically the same size as the original dataset.\n",
        "\n",
        "4. **Repeat:**\n",
        "   - Repeat the process (steps 2 and 3) multiple times to create multiple bootstrap samples. The number of iterations is determined by the desired number of bootstrap samples.\n",
        "\n",
        "5. **Estimation and Inference:**\n",
        "   - Use each bootstrap sample to estimate population parameters or assess the variability of a statistic. For example, compute the mean, variance, or confidence intervals based on the bootstrap samples.\n",
        "\n",
        "The main purpose of bootstrap sampling is to approximate the distribution of a statistic or parameter by resampling from the observed data. This technique is particularly useful when analytical methods for estimating the distribution are challenging or unavailable.\n",
        "\n",
        "Key points about bootstrap sampling:\n",
        "\n",
        "- **Resampling with Replacement:** Each observation in the original dataset has an equal probability of being selected in each iteration. This allows for the creation of diverse bootstrap samples.\n",
        "\n",
        "- **Size of Bootstrap Sample:** The size of each bootstrap sample is typically the same as the size of the original dataset (\\(n\\)).\n",
        "\n",
        "- **Applications:** Bootstrap sampling is widely used for estimating standard errors, confidence intervals, and making statistical inferences in situations where the underlying distribution is not well-known or assumptions are violated.\n",
        "\n",
        "- **Bootstrap Confidence Intervals:** Bootstrap samples can be used to compute confidence intervals for parameters or statistics. This provides a non-parametric approach to statistical inference.\n",
        "\n",
        "Bootstrap sampling is a versatile and powerful technique in statistics and machine learning, especially when dealing with small or complex datasets. It is commonly used in conjunction with methods like bootstrapped aggregation (bagging) and in the estimation of uncertainty in predictive modeling."
      ],
      "metadata": {
        "id": "5S_4aXAjcHCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Bootstrap Aggregation (Bagging)**\n",
        "  \n",
        "  is an ensemble machine learning technique that aims to improve the stability and accuracy of models by combining multiple base models trained on different subsets of the training data. It is particularly effective in reducing overfitting and variance in the predictions. The most common application of bagging is in constructing random forests, although it can be applied to other base learners as well.\n",
        "\n",
        "  ![alt](https://www.simplilearn.com/ice9/free_resources_article_thumb/Bagging.PNG)\n",
        "\n",
        "Here are the key components and principles of Bootstrap Aggregation:\n",
        "\n",
        "1. **Bootstrap Sampling:**\n",
        "   - Bagging involves creating multiple bootstrap samples from the original training dataset. A bootstrap sample is obtained by randomly sampling with replacement from the training data. Each bootstrap sample is of the same size as the original dataset, but it may contain duplicate instances and exclude some original instances.\n",
        "\n",
        "2. **Base Model Training:**\n",
        "   - A base model (learner) is trained on each bootstrap sample independently. The base models can be of any type, but decision trees are commonly used.\n",
        "\n",
        "3. **Parallel Training:**\n",
        "   - The training of base models is typically done in parallel, allowing for efficient use of computational resources.\n",
        "\n",
        "4. **Aggregation:**\n",
        "   - The predictions of individual base models are aggregated to form the final ensemble prediction. The method of aggregation depends on the task:\n",
        "      - For classification: Voting or averaging of class probabilities.\n",
        "      - For regression: Averaging of individual predictions.\n",
        "\n",
        "5. **Reduction of Variance:**\n",
        "   - Bagging helps reduce the variance of the model by introducing diversity among the base models. This is achieved by training each base model on a slightly different dataset due to the randomness introduced by bootstrap sampling.\n",
        "\n",
        "\n",
        "\n",
        "7. **Out-of-Bag (OOB) Evaluation:**\n",
        "   - As each base model is trained on a bootstrap sample, there are instances not included in its training set. These out-of-bag instances can be used for model evaluation without the need for a separate validation set.\n",
        "\n",
        "8. **Application to Various Models:**\n",
        "   - While bagging is commonly associated with decision trees and Random Forests, it can be applied to various base models, including linear models, support vector machines, and more.\n",
        "\n",
        "Bagging is effective when the base models are unstable or have high variance. By combining multiple models trained on different subsets of data, bagging improves generalization and robustness, making it a powerful technique in ensemble learning."
      ],
      "metadata": {
        "id": "GVMKQytubJky"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CC9MPbiQb5eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Boosting**\n",
        "\n",
        "  is an ensemble machine learning technique that aims to improve the predictive performance of a model by combining the predictions of multiple weak learners. Unlike bagging, where models are trained independently and their predictions are averaged or voted upon, boosting trains models sequentially, with each new model focusing on correcting the errors made by the previous ones.\n",
        "\n",
        "Here are the key concepts and principles of boosting:\n",
        "\n",
        "1. **Weak Learners:**\n",
        "   - Boosting typically uses weak learners as base models. A weak learner is a model that performs slightly better than random chance.\n",
        "\n",
        "2. **Sequential Training:**\n",
        "   - Models are trained sequentially, and each subsequent model focuses on the mistakes made by the ensemble of models trained so far.\n",
        "\n",
        "3. **Weighted Training Instances:**\n",
        "   - During training, instances that are misclassified by the previous models are assigned higher weights. This gives more emphasis to the difficult-to-classify instances in the subsequent models.\n",
        "\n",
        "4. **Model Weighting:**\n",
        "   - Each model is assigned a weight based on its accuracy. Models with higher accuracy are given more weight in the final prediction.\n",
        "\n",
        "5. **Adaptive Learning Rate:**\n",
        "   - Boosting algorithms often use an adaptive learning rate, adjusting the contribution of each model based on its performance. This helps prevent overshooting and oscillations.\n",
        "\n",
        "6. **Common Boosting Algorithms:**\n",
        "   - Some well-known boosting algorithms include:\n",
        "     - **AdaBoost (Adaptive Boosting):** Focuses on correcting misclassifications by assigning higher weights to misclassified instances.\n",
        "     - **Gradient Boosting:** Minimizes the errors of the previous models by fitting new models to the residuals. Common implementations include XGBoost, LightGBM, and CatBoost.\n",
        "     - **Stochastic Gradient Boosting (SGD):** Applies gradient boosting in a stochastic manner, using random subsets of data for each iteration.\n",
        "\n",
        "7. **Final Prediction:**\n",
        "   - The final prediction is a weighted combination of the predictions of all the weak learners. Models with higher accuracy contribute more to the final prediction.\n",
        "\n",
        "8. **Reducing Bias and Variance:**\n",
        "   - Boosting aims to reduce both bias and variance. It reduces bias by sequentially correcting mistakes, and it reduces variance by combining diverse models.\n",
        "\n",
        "9. **Robust to Overfitting:**\n",
        "   - Boosting is less prone to overfitting compared to training a single complex model. The emphasis on misclassified instances during training helps the model generalize well to unseen data.\n",
        "\n",
        "10. **Feature Importance:**\n",
        "    - Boosting algorithms often provide insights into feature importance, indicating which features are more influential in making accurate predictions.\n",
        "\n",
        "Boosting is widely used in practice and has been successful in various machine learning applications, including classification, regression, and ranking. It has become a fundamental technique and is implemented in popular machine learning libraries."
      ],
      "metadata": {
        "id": "7y3nRURnc3vS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZKwzyxqcdAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}